{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment object with file_name as None after you run this block click on play button in Unity Editor to connect this with the agent in the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a non-blocking call that only loads the environment.\n",
    "channel = EngineConfigurationChannel()\n",
    "\n",
    "env = UnityEnvironment(file_name=None,  seed=1, side_channels=[channel])\n",
    "\n",
    "channel.set_configuration_parameters(time_scale = 4.0) # This helps to speed up the environment N times\n",
    "# Start interacting with the evironment.\n",
    "env.reset()\n",
    "behavior_names = env.behavior_specs.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARAI?team=0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( env.behavior_specs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "behavior_name is important to assign action to an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only consider the first Behavior\n",
    "behavior_name = list(env.behavior_specs)[0] \n",
    "print(f\"Name of the behavior : {behavior_name}\")\n",
    "spec = env.behavior_specs[behavior_name]\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  2\n",
      "Is there a visual observation ? False\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(spec.observation_shapes))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "# Visual observation have 3 dimensions: Height, Width and number of channels\n",
    "vis_obs = any(len(shape) == 3 for shape in spec.observation_shapes)\n",
    "print(\"Is there a visual observation ?\", vis_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action is continuous\n",
      "There are 2 action(s)\n"
     ]
    }
   ],
   "source": [
    "# Is the Action continuous or multi-discrete ?\n",
    "if spec.is_action_continuous():\n",
    "    print(\"The action is continuous\")\n",
    "if spec.is_action_discrete():\n",
    "    print(\"The action is discrete\")\n",
    "\n",
    "# How many actions are possible ?\n",
    "print(f\"There are {spec.action_size} action(s)\")\n",
    "\n",
    "# For discrete actions only : How many different options does each action has ?\n",
    "if spec.is_action_discrete():\n",
    "    for action, branch_size in enumerate(spec.discrete_action_branches):\n",
    "        print(f\"Action number {action} has {branch_size} different options\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[1.       , 1.       , 0.       , 0.2348219, 0.       , 0.2639548]],\n",
       "        dtype=float32),\n",
       "  array([[-6.1960000e-01,  0.0000000e+00,  2.1900000e-01, -8.8757544e-05,\n",
       "          -3.3263044e-04]], dtype=float32)],\n",
       " <mlagents_envs.base_env.TerminalSteps at 0x1b5bea10808>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "This is just to show the output, You can see that for single agent we have two arrays as observation because\n",
    "first array is the information provide by raycast and second one is input provided by us in the script \n",
    "that's why we later used numpy.hstack() to merge them as 1 observation. \n",
    "\"\"\"\n",
    "env.reset()\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "decision_steps.obs, terminal_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to show the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(50):\n",
    "    env.reset()\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    #print(list(decision_steps),list(terminal_steps),behavior_name)\n",
    "    print(len(decision_steps),len(terminal_steps))\n",
    "    tracked_agent = -1 # -1 indicates not yet tracking\n",
    "    done = False # For the tracked_agent\n",
    "    episode_rewards = 0 # For the tracked_agent\n",
    "    while not done:\n",
    "        # Track the first agent we see if not tracking \n",
    "        # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "        #print(\"____________________________________\")\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0] \n",
    "        print(list(decision_steps),list(terminal_steps),tracked_agent)\n",
    "        \n",
    "        # Generate an action for all agents\n",
    "        action = spec.create_random_action(len(decision_steps))\n",
    "        #print(\"Action\",action)\n",
    "        # Set the actions\n",
    "        env.set_actions(behavior_name, action)\n",
    "\n",
    "        # Move the simulation forward\n",
    "        env.step()\n",
    "\n",
    "        # Get the new simulation results\n",
    "        \n",
    "        #print(decision_steps.obs)\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        #print(list(decision_steps),list(terminal_steps))\n",
    "        #print(decision_steps.obs)\n",
    "        if tracked_agent in decision_steps: # The agent requested a decision\n",
    "            episode_rewards += decision_steps[tracked_agent].reward\n",
    "        if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "            episode_rewards += terminal_steps[tracked_agent].reward\n",
    "            done = True\n",
    "    print(f\"Total rewards for episode {episode} is {episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TensorFlow models for continuous action using normal distribution and critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Continuous(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Continuous, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.mu = tf.keras.layers.Dense(2, activation=\"tanh\")\n",
    "        self.sig = tf.keras.layers.Dense(2, activation=\"softplus\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1 = self.dense1(inputs)\n",
    "\n",
    "        x1 = self.dense2(x1)\n",
    "        mu = self.mu(x1)\n",
    "        sig = self.sig(x1)\n",
    "        \n",
    "        mu = tf.squeeze(mu)\n",
    "        sig = tf.squeeze(sig) + 0.00001\n",
    "        return tf.convert_to_tensor([mu,sig])\n",
    "\n",
    "\n",
    "modelA = Continuous()   \n",
    "\n",
    "modelV = tf.keras.Sequential()\n",
    "modelV.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "modelV.add(tf.keras.layers.Dense(128, activation = \"relu\"))\n",
    "modelV.add(tf.keras.layers.Dense(1, activation = \"linear\"))\n",
    "modelV.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.003))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case if we have to resume the training from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = tf.keras.models.load_model('Car/actor')\n",
    "modelV = tf.keras.models.load_model('Car/critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, state, env , modelActor, modelV , episodes=300, gamma = 0.99):\n",
    "        self.state = state\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.modelActor = modelActor\n",
    "        self.modelV = modelV\n",
    "        self.gamma = gamma\n",
    "        self.lr =  0.0007\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr)\n",
    "\n",
    "    def record(self):\n",
    "        scores, episodes = [], []\n",
    "        for i in range(self.episodes):\n",
    "            if i % 20 == 0:\n",
    "                self.lr = self.lr * 0.99\n",
    "                self.optimizer = tf.keras.optimizers.Adam(learning_rate = self.lr)\n",
    "                modelV.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(self.lr))\n",
    "\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            #print(list(decision_steps),list(terminal_steps))\n",
    "            tracked_agent = -1 # -1 indicates not yet tracking\n",
    "            done = False # For the tracked_agent\n",
    "            episode_rewards = 0 # For the tracked_agent\n",
    "            reward = 0\n",
    "            while not done: \n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Track the first agent we see if not tracking \n",
    "                    # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0] \n",
    "                    elif tracked_agent == -1:\n",
    "                        break\n",
    "                    observation = np.hstack((decision_steps.obs[0],decision_steps.obs[1]))\n",
    "                    # Generate an action for all agents\n",
    "                    logits = self.modelActor(observation)                     \n",
    "                    norm = tf.compat.v1.distributions.Normal(loc=logits[0], scale=logits[1])                    \n",
    "                    sample = norm.sample([1])                   \n",
    "                    decision = tf.clip_by_value(sample , -1.0 , 1.0) \n",
    "                    action = tf.squeeze(decision).numpy().reshape(-1,2)\n",
    "                    \n",
    "                    #action = spec.create_random_action(len(decision_steps))\n",
    "                    \n",
    "                    #print(action)\n",
    "\n",
    "                    # Set the actions\n",
    "                    env.set_actions(behavior_name, action)\n",
    "\n",
    "                    # Move the simulation forward\n",
    "                    env.step()\n",
    "\n",
    "                    # Get the new simulation results\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                    #print(list(decision_steps),list(terminal_steps),tracked_agent)\n",
    "                    #print(np.array(decision_steps.obs).reshape(-1,8) , np.array(terminal_steps.obs).reshape(-1,8))\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    if tracked_agent in decision_steps: # The agent requested a decision\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        episode_rewards += decision_steps[tracked_agent].reward\n",
    "                        newObservation = np.hstack((decision_steps.obs[0],decision_steps.obs[1]))                        \n",
    "                    if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                        newObservation = np.hstack((terminal_steps.obs[0],terminal_steps.obs[1]))\n",
    "                        done = True\n",
    "                    #print(observation, \"new \",newObservation)\n",
    "                    \n",
    "                    prob =    norm.log_prob(decision) \n",
    "                    entropy = norm.entropy() * 0.01                    \n",
    "                     \n",
    "                    value = self.modelV(observation)\n",
    "                    value_ = self.modelV(newObservation)\n",
    "                    delta = reward + self.gamma * value_ * (1 - done)\n",
    "                    advantage = delta - value\n",
    "                    loss = - (prob * advantage + 0.01 * entropy)\n",
    "                grads = tape.gradient(loss, self.modelActor.trainable_variables)      \n",
    "                #print(\"delta \", delta)\n",
    "                self.modelV.fit(observation, delta,epochs = 1, batch_size=1, verbose=0)                \n",
    "                self.optimizer.apply_gradients(zip(grads, self.modelActor.trainable_variables))      \n",
    "                observation = newObservation\n",
    "                \n",
    "            scores.append(episode_rewards)\n",
    "            episodes.append(i)\n",
    "            print(\"Episode: {}, Cumulative reward: {:0.2f}\".format(i, episode_rewards))\n",
    "\n",
    "        plt.plot(episodes, scores, 'b') \n",
    "        plt.show()\n",
    "\n",
    "modelV.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.003))\n",
    "\n",
    "obj = ActorCritic(state=[3], env=env, modelActor=modelA, modelV=modelV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "obj.record() \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA.save('Car/actor') \n",
    "modelV.save('Car/critic') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
